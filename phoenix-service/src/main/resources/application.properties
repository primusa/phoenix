server.port=8080
spring.application.name=phoenix-service



# Metrics Configuration (Direct OTLP Push)
# Note: This sends app metrics directly to OTel, distinct from Prometheus scraping Actuator
management.otlp.metrics.export.url=http://otel-collector:4318/v1/metrics
management.otlp.metrics.export.step=10s



# Logging
# Database initialization
spring.sql.init.mode=always
logging.level.com.example.phoenix=DEBUG
logging.level.org.springframework.kafka=INFO
logging.level.org.apache.kafka=INFO


# Observability
# Tracing Configuration
management.tracing.sampling.probability=1.0
management.opentelemetry.tracing.export.otlp.endpoint=http://otel-collector:4318/v1/traces
# Actuator Exposure (For Prometheus scraping /actuator/prometheus)
management.endpoints.web.exposure.include=health,info,prometheus,env
management.endpoint.prometheus.enabled=true
management.metrics.export.prometheus.enabled=true
management.observations.key-values.enabled=true
management.tracing.events.enabled=true

# Kafka Main Config
spring.kafka.consumer.group-id=phoenix-group
spring.kafka.consumer.auto-offset-reset=earliest
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer

# Robustness: Allow startup without real keys (Ollama is default)
spring.ai.openai.api-key=${OPENAI_API_KEY:sk-dummy-key-to-allow-startup}
spring.ai.vertex.ai.gemini.project-id=${GEMINI_PROJECT_ID:dummy-project}
spring.ai.vertex.ai.gemini.location=${GEMINI_LOCATION:us-central1}


# Enable OTLP Logging
# Logging Configuration (Ensures Logback sends logs via OTLP)
management.opentelemetry.logging.export.otlp.endpoint=http://otel-collector:4318/v1/logs
management.opentelemetry.instrumentation.logback-appender.enabled=true
# Enable Structured Logging for the Console (Optional but recommended)
# This makes logs easy for tools like Jaeger/Loki to parse
logging.structured.format.console=otlp
management.opentelemetry.logging.export.enabled=true
#management.otlp.metrics.export.url=http://jaeger:4318/v1/metrics
#If running standalone Docker:
#Use the special DNS name that points back to your host machine:
#management.otlp.metrics.export.url=http://host.docker.internal:4318/v1/metrics

# Spring Boot 4 unified OTLP endpoint (Works for Traces, Metrics, and Logs)
# management.opentelemetry.metrics.export.otlp.endpoint=http://jaeger:4318/v1/metrics

# If you want to switch to the faster gRPC transport (recommended for SB4), use:
# management.opentelemetry.metrics.export.otlp.transport=grpc
# management.opentelemetry.metrics.export.otlp.endpoint=http://jaeger:4317




spring.ai.vertex.ai.gemini.chat.options.model=gemini-1.5-pro-001

spring.ai.vertex.ai.embedding.project-id=${GEMINI_PROJECT_ID:dummy-project}
spring.ai.vertex.ai.embedding.location=${GEMINI_LOCATION:us-central1}
spring.ai.vertex.ai.embedding.text.options.model=text-embedding-005

# application-dev.yml
#spring:
#ai:
#openai:
#base-url: http://localhost:11434/v1
#api-key: ollama
#chat:
#model: llama3:3b




spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.model=tinyllama
#spring.ai.ollama.chat.model=llama3:3b

#spring.ai.ollama.chat.options.model=llama3:3b
#spring.ai.ollama.chat.options.model=llama3
# Crucial: Include embedding models in the auto-pull task
spring.ai.ollama.init.embedding.include=true

# Ensure the model name exactly matches what Ollama expects
spring.ai.ollama.embedding.options.model=nomic-embed-text
#spring.ai.ollama.embedding.options.model=mxbai-embed-large

# Strategy: pull the model only if it's not already in Ollama
spring.ai.ollama.init.pull-model-strategy=when_missing

# Increase the timeout because large models (mxbai is ~670MB) take time to download
spring.ai.ollama.init.timeout=10m




spring.ai.openai.chat.options.model=gpt-4o




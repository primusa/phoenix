server.port=8080
spring.application.name=phoenix-service
# Enterprise Concurrency (Project Loom)
spring.threads.virtual.enabled=true


# Metrics Configuration (Direct OTLP Push)
# Note: This sends app metrics directly to OTel, distinct from Prometheus scraping Actuator
management.otlp.metrics.export.url=http://localhost:4318/v1/metrics
management.otlp.metrics.export.step=10s

# --- Kafka ---
spring.kafka.bootstrap-servers=localhost:9092
spring.kafka.properties.enable.metrics.push=false

# --- Spring AI / Weaviate / Ollama ---
phoenix.vector-store.mode=weaviate
spring.ai.vectorstore.weaviate.host=localhost:8080
spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.model=tinyllama

# --- Datasource (PostgreSQL) ---
spring.datasource.url=jdbc:postgresql://localhost:5432/insurance_corp
spring.datasource.username=postgres
spring.datasource.password=postgres
spring.datasource.type=com.zaxxer.hikari.HikariDataSource

# --- HikariCP Optimization ---
# Maximum number of idle connections in the pool
spring.datasource.hikari.minimum-idle=5
# Maximum number of actual connections to the database
spring.datasource.hikari.maximum-pool-size=15
# Idle timeout for connections in the pool (5 minutes)
spring.datasource.hikari.idle-timeout=300000
# Maximum lifetime of a connection in the pool (30 minutes)
spring.datasource.hikari.max-lifetime=1800000
# Connection timeout when waiting for a connection from the pool (20 seconds)
spring.datasource.hikari.connection-timeout=20000
# Pool name for monitoring
spring.datasource.hikari.pool-name=PhoenixHikariPool

# --- OTLP Pipeline ---
management.otlp.tracing.endpoint=http://localhost:4318/v1/traces
otel.dotnet.auto.logs.include.formatted.message=false
otel.instrumentation.common.default.enabled=false

# --- Tracing (Spring Boot 3.4+ / SB4 style) ---
management.opentelemetry.tracing.export.otlp.endpoint=http://localhost:4318/v1/traces
management.tracing.sampling.probability=1.0

# --- Metrics ---
#management.otlp.metrics.export.url=http://localhost:4318/v1/metrics
#management.otlp.metrics.export.step=10s

# --- Logs ---
management.opentelemetry.logging.export.otlp.endpoint=http://localhost:4318/v1/logs
management.opentelemetry.logging.export.enabled=false
management.opentelemetry.instrumentation.logback.appender.enabled=false
logging.structured.format.console=otlp


# Logging
# Database initialization


spring.sql.init.mode=never
spring.jpa.hibernate.ddl-auto=none
spring.flyway.enabled=true
spring.flyway.clean-disabled=false
spring.flyway.locations=classpath:db/migration
spring.flyway.baseline-on-migrate=true
spring.flyway.baseline-version=0
spring.flyway.out-of-order=true
spring.flyway.repair-on-migrate=true
spring.flyway.schemas=public
spring.flyway.validate-on-migrate=true
spring.flyway.table=flyway_schema_history

logging.level.com.example.phoenix=DEBUG
logging.level.org.springframework.kafka=INFO
logging.level.org.apache.kafka=INFO
logging.level.org.flywaydb=DEBUG


# Observability
# Tracing Configuration
#management.tracing.sampling.probability=1.0
#management.opentelemetry.tracing.export.otlp.endpoint=http://localhost:4318/v1/traces
# Actuator Exposure (For Prometheus scraping /actuator/prometheus)
management.endpoints.web.exposure.include=health,info,prometheus,env
management.endpoint.prometheus.enabled=true
management.metrics.export.prometheus.enabled=true
management.observations.key-values.enabled=true
management.tracing.events.enabled=true

# Kafka Main Config
spring.kafka.consumer.group-id=phoenix-group
spring.kafka.consumer.auto-offset-reset=earliest
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer

# Robustness: Allow startup without real keys (Ollama is default)
spring.ai.openai.api-key=${OPENAI_API_KEY:sk-dummy-key-to-allow-startup}
spring.ai.vertex.ai.gemini.project-id=${GEMINI_PROJECT_ID:dummy-project}
spring.ai.vertex.ai.gemini.location=${GEMINI_LOCATION:us-central1}


# Enable OTLP Logging
# Logging Configuration (Ensures Logback sends logs via OTLP)
#management.opentelemetry.logging.export.otlp.endpoint=http://localhost:4318/v1/logs
#management.opentelemetry.instrumentation.logback-appender.enabled=true
# Enable Structured Logging for the Console (Optional but recommended)
# This makes logs easy for tools like Jaeger/Loki to parse
#logging.structured.format.console=otlp
#management.opentelemetry.logging.export.enabled=true
#management.otlp.metrics.export.url=http://jaeger:4318/v1/metrics
#If running standalone Docker:
#Use the special DNS name that points back to your host machine:
#management.otlp.metrics.export.url=http://host.docker.internal:4318/v1/metrics

# Spring Boot 4 unified OTLP endpoint (Works for Traces, Metrics, and Logs)
# management.opentelemetry.metrics.export.otlp.endpoint=http://jaeger:4318/v1/metrics

# If you want to switch to the faster gRPC transport (recommended for SB4), use:
# management.opentelemetry.metrics.export.otlp.transport=grpc
# management.opentelemetry.metrics.export.otlp.endpoint=http://jaeger:4317


spring.ai.vertex.ai.gemini.chat.options.model=gemini-1.5-pro-001

spring.ai.vertex.ai.embedding.project-id=${GEMINI_PROJECT_ID:dummy-project}
spring.ai.vertex.ai.embedding.location=${GEMINI_LOCATION:us-central1}
spring.ai.vertex.ai.embedding.text.options.model=text-embedding-005

# application-dev.yml
#spring:
#ai:
#openai:
#base-url: http://localhost:11434/v1
#api-key: ollama
#chat:
#model: llama3:3b




#spring.ai.ollama.base-url=http://localhost:11434
#spring.ai.ollama.chat.model=tinyllama
#spring.ai.ollama.chat.model=llama3:3b

#spring.ai.ollama.chat.options.model=llama3:3b
#spring.ai.ollama.chat.options.model=llama3
# Crucial: Include embedding models in the auto-pull task
spring.ai.ollama.init.embedding.include=true

# Ensure the model name exactly matches what Ollama expects
spring.ai.ollama.embedding.options.model=nomic-embed-text
#spring.ai.ollama.embedding.options.model=mxbai-embed-large

# Strategy: pull the model only if it's not already in Ollama
spring.ai.ollama.init.pull-model-strategy=when_missing

# Increase the timeout because large models (mxbai is ~670MB) take time to download
spring.ai.ollama.init.timeout=10m




spring.ai.openai.chat.options.model=gpt-4o




services:
  # 1. Database (PostgreSQL) - The source of truth
  legacy-db:
    profiles: [ "local" ]
    image: postgres:15-alpine
    ports: [ "5432:5432" ]
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: insurance_corp
    command:
      - "postgres"
      - "-c"
      - "wal_level=logical" # Required for CDC
      - "-c"
      - "max_replication_slots=5" # Allows Debezium to create a slot
      - "-c"
      - "max_wal_senders=5" # Allows the background process to send data
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U postgres -d insurance_corp" ]
      interval: 5s
      timeout: 5s
      retries: 5

  # 2. Kafka KRaft (Latest 8.1 / Kafka 4.1 equivalent). The Bridge (Debezium + Kafka)
  kafka:
    image: confluentinc/cp-kafka:8.1.0
    container_name: kafka
    shm_size: 2g
    ports:
      - "29092:29092" # Internal PLAINTEXT_HOST
    #    - "9405:9405" # Prometheus JMX agent
    volumes:
      # - ./jmx_prometheus_javaagent.jar:/usr/share/java/kafka/jmx_prometheus_javaagent.jar
      - ./kafka-jmx-config.yml:/etc/kafka/kafka-jmx-config.yml
    environment:
      # Kafka node & cluster configuration
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka:9093'
      KAFKA_LISTENERS: 'PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093,PLAINTEXT_HOST://0.0.0.0:29092'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
      CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

      # JVM / memory settings
      KAFKA_HEAP_OPTS: "-Xmx400M -Xms400M"
      #   KAFKA_JVM_PERFORMANCE_OPTS: "-javaagent:/usr/share/java/kafka/jmx_prometheus_javaagent.jar=9405:/etc/kafka/kafka-jmx-config.yml"

      # Redirect temp files to a location with enough space
      KAFKA_OPTS: "-Djava.io.tmpdir=/tmp/java"

    # Ensure the temp directory exists in container
    command: [ "bash", "-c", "mkdir -p /tmp/java && /etc/confluent/docker/run" ]

    healthcheck:
      # We use 9092 because it is the internal PLAINTEXT listener
      test: [ "CMD-SHELL", "kafka-topics --bootstrap-server localhost:9092 --list || exit 1" ]
      interval: 10s # Check every 10s
      timeout: 10s # Give the command 10s to respond
      retries: 10 # Be patient (total wait up to 100s)
      start_period: 20s # Don't even start checking for 20 seconds

  #  schema-registry:
  #    image: confluentinc/cp-schema-registry:7.6.0
  #    depends_on: [ kafka ]
  #    ports: [ "8081:8081" ]
  #    environment:
  #      SCHEMA_REGISTRY_HOST_NAME: schema-registry
  #      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:9092
  #      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081

  # 3. CDC & Connect
  debezium:
    image: quay.io/debezium/connect:2.5 # Using a specific version
    container_name: debezium
    depends_on:
      kafka:
        condition: service_healthy # Wait for Kafka to be fully functional, not just 'started'
    ports:
      - "8083:8083"
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      GROUP_ID: "debezium-cluster"
      CONFIG_STORAGE_TOPIC: "debezium_configs"
      OFFSET_STORAGE_TOPIC: "debezium_offsets"
      STATUS_STORAGE_TOPIC: "debezium_statuses"
      CONFIG_STORAGE_REPLICATION_FACTOR: 1 # Stores the last "Position" (LSN) read from Postgres.	Debezium restarts from the beginning of time (re-reads everything).
      OFFSET_STORAGE_REPLICATION_FACTOR: 1 # Stores the connector JSON settings.	You have to POST your connector config every single time you restart.
      STATUS_STORAGE_REPLICATION_FACTOR: 1 # Stores the current state (RUNNING/FAILED).	Monitoring tools (and your JMX agent) won't show the correct status.
      # Fast Boot Optimizations
      KAFKA_HEAP_OPTS: "-Xmx400M -Xms400M"
      # Suppress Telemetry
      CONNECT_PRODUCER_ENABLE_METRICS_PUSH: "false"
      CONNECT_CONSUMER_ENABLE_METRICS_PUSH: "false"

  # 3. Modern Destination (Weaviate Vector DB)
  weaviate:
    image: semitechnologies/weaviate:latest
    ports: [ "8090:8080" ]
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      DEFAULT_VECTORIZER_MODULE: 'text2vec-openai'

  # 4. Local AI (Ollama) - Optional
  ollama:
    # profiles: [ "local" ]
    image: alpine/ollama:latest
    # For local development, you can build the Ollama image with your custom models and use that instead. 
    # Just ensure the image is tagged as 'alpine/ollama:latest' or update the reference here.
    #image: ollama/ollama:latest
    ports: [ "11434:11434" ]
    entrypoint: /bin/sh
    volumes:
      - ollama_data:/root/.ollama
    dns: [ 8.8.8.8 ]
    # environment:
    #   - OLLAMA_KEEP_ALIVE=-1m
    command: >
      -c " ollama serve & sleep 5 && ollama pull tinyllama && ollama pull nomic-embed-text && wait "
    # ollama run tinyllama 'ping' --keepalive -1m &&
    # ollama run nomic-embed-text 'ping' --keepalive -1m &&
    restart: unless-stopped

  # 5. Observability (Jaeger + Prometheus)
  # --- EXTERNAL METRICS: Prometheus ---
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    ports: [ "9090:9090" ]

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    restart: unless-stopped
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    ports:
      - "9100:9100"

  # --- BACKEND : JAEGER v2.0 (Deep-Dive Tracing) ---
  jaeger:
    image: jaegertracing/jaeger:latest
    ports:
      - "16686:16686" # UI
      # used by otel-collector. So commenting ports 4317 & 4318 out
      #  - "4317:4317" # OTLP gRPC
      # - "4318:4318" # OTLP HTTP
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - JAEGER_DISABLED=false
      - METRICS_STORAGE_TYPE=prometheus
      - PROMETHEUS_SERVER_URL=http://prometheus:9090
      - PROMETHEUS_QUERY_NAMESPACE=span_metrics
      - PROMETHEUS_QUERY_DURATION_UNIT=ms
      - PROMETHEUS_QUERY_LABEL_OPERATION=operation
      - PROMETHEUS_QUERY_LABEL_SERVICE=service
      - MONITOR_MENU_ENABLED=true
      - COLLECTOR_OTLP_ENABLED=true
      # --- ALLOW IFRAME EMBEDDING ---
      - QUERY_BASE_PATH=/jaeger # Ensures paths resolve correctly in sub-frames

  # --- BACKEND : LGTM Stack (Unified Observability) ---
  lgtm:
    image: grafana/otel-lgtm:latest
    ports:
      - "3000:3000" # Grafana UI
      #  - "4317:4317"
      - "4319:4318"
      # --- EXPOSED DIRECT PORTS ---
      - "3100:3100" # Loki (API & Direct Log Ingestion)
      - "3200:3200" # Tempo (API & Direct Trace Ingestion)
      - "9091:9090" # Prometheus (Optional: Direct Query API)
    # We remove 4317/4318 from LGTM to avoid conflict with the standalone Collector
    environment:
      # --- GRAFANA IFRAME & SECURITY CONFIG ---
      - GF_SECURITY_ALLOW_EMBEDDING=true
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_AUTH_DISABLE_LOGIN_FORM=true
      # Prevents "Cookie not found" errors inside iframes
      - GF_SECURITY_COOKIE_SAMESITE=none
      - GF_SECURITY_COOKIE_SECURE=false
      - GF_HTTP_ENFORCE_DOMAIN=false
      - GF_SECURITY_CORS_ALLOW_ALL=true # For dev; in prod, set to your domain
      # --- SUBPATH CONFIG (Essential for API Gateway/ALB) ---
      - GF_SERVER_ROOT_URL=%(protocol)s://%(domain)s:%(http_port)s/grafana/
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
      # --- COOKIE HANDLING (Essential for Cloud/HTTPS) ---
      - GF_SECURITY_COOKIE_SAMESITE=none
      - GF_SECURITY_COOKIE_SECURE=true # Set to true if using HTTPS in cloud
      # Path to the JSON files inside the container
      - GF_DASHBOARDS_DEFAULT_HOME_DASHBOARD_PATH=/var/lib/grafana/dashboards/jvm_otlp.json
      # --- MAXIMUM DATA: show more points and allow larger time ranges ---
      - GF_EXPLORE_QUERY_DS_MAX_POINTS=10000
      - GF_DATAPROXY_MAX_RESPONSE_BODY_SIZE=104857600
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml
      # Mount datasource provisioning (override Prometheus -> http://prometheus:9090)
      - ./grafana/provisioning/datasources:/etc/grafana/provisioning/datasources
      # Ensure LGTM's *internal* Prometheus scrapes our app + collector metrics
      - ./grafana/lgtm-prometheus.yaml:/otel-lgtm/prometheus.yaml
      # Mount provisioning config
      - ./grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
      # Mount actual JSON dashboard files
      - ./grafana/dashboards:/var/lib/grafana/dashboards

  # Replacement for LGTM in AWS (Lighter)
  grafana:
    profiles: [ "aws" ]
    image: grafana/grafana:10.4.0
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ALLOW_EMBEDDING=true
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_AUTH_DISABLE_LOGIN_FORM=true
      - GF_SECURITY_COOKIE_SAMESITE=none
      - GF_SECURITY_COOKIE_SECURE=false
      - GF_SERVER_ROOT_URL=%(protocol)s://%(domain)s:%(http_port)s/grafana/
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
    volumes:
      - ./grafana/provisioning/datasources:/etc/grafana/provisioning/datasources
      - ./grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/dashboards:/var/lib/grafana/dashboards
    depends_on: [ prometheus ]

  # --- THE BRAIN: OpenTelemetry Collector ---
  otel-collector:
    image: otel/opentelemetry-collector-contrib:latest
    command: [ "--config=/etc/otel-collector-config.yaml" ]
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml
    ports:
      - "4317:4317" # OTLP gRPC
      - "4318:4318" # OTLP HTTP
      - "8889:8889" # Prometheus exporter
    depends_on: [ prometheus ]

  # 6. Backend Service
  phoenix-backend:
    build: ./phoenix-service
    ports: [ "8080:8080" ]
    environment:
      SPRING_KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      # Fix: Disable internal Kafka telemetry to use OTel agent instead
      SPRING_KAFKA_PROPERTIES_ENABLE_METRICS_PUSH: "false"
      SPRING_AI_VECTORSTORE_WEAVIATE_HOST: weaviate:8080
      SPRING_AI_OLLAMA_BASE_URL: http://ollama:11434
      SPRING_DATASOURCE_URL: jdbc:postgresql://legacy-db:5432/insurance_corp
      SPRING_DATASOURCE_USERNAME: postgres
      SPRING_DATASOURCE_PASSWORD: postgres
      # OTel Pipeline
      MANAGEMENT_OTLP_TRACING_ENDPOINT: http://otel-collector:4318/v1/traces
      OTEL_DOTNET_AUTO_LOGS_INCLUDE_FORMATTED_MESSAGE: true
      OTEL_INSTRUMENTATION_COMMON_DEFAULT_ENABLED: true
      SPRING_AI_OLLAMA_CHAT_MODEL: tinyllama

      # --- TRACING ---
      # Modern SB4 standard for tracing export
      MANAGEMENT_OPENTELEMETRY_TRACING_EXPORT_OTLP_ENDPOINT: http://otel-collector:4318/v1/traces
      MANAGEMENT_TRACING_SAMPLING_PROBABILITY: "1.0"

      # --- METRICS ---
      # This powers the Micrometer OTLP push to the collector
      # Use the SERVICE NAME 'otel-collector' for internal networking
      MANAGEMENT_OTLP_METRICS_EXPORT_URL: http://otel-collector:4318/v1/metrics
      MANAGEMENT_OTLP_METRICS_EXPORT_STEP: "10s"

      # --- LOGS ---
      # Modern SB4 standard for logging export]
      MANAGEMENT_OPENTELEMETRY_LOGGING_EXPORT_OTLP_ENDPOINT: http://otel-collector:4318/v1/logs
      MANAGEMENT_OPENTELEMETRY_LOGGING_EXPORT_ENABLED: "true"
      MANAGEMENT_OPENTELEMETRY_INSTRUMENTATION_LOGBACK_APPENDER_ENABLED: "true"
      LOGGING_STRUCTURED_FORMAT_CONSOLE: "otlp"

    depends_on: [ kafka, weaviate, otel-collector ]

  # 7. Frontend UI
  phoenix-frontend:
    build: ./phoenix-ui
    ports: [ "5173:8080" ] # Map host 5173 to container 8080
    depends_on: [ phoenix-backend ]

volumes:
  ollama_data:
